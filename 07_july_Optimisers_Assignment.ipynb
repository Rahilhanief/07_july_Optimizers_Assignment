{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d441e119-2983-44ca-8de1-13c7d1483fa8",
   "metadata": {},
   "source": [
    "Part 1: Understanding Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f15d3d7-9ab5-4d9e-b265-9f4f0ea4d56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe process of minimizing (or maximizing) any mathematical expression is called optimization.\\nOptimizers are algorithms or methods used to change the attributes of the neural network such as weights and \\nlearning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function.\\n\\nThe process of optimisation aims to lower the risk of errors or loss from these predictions, \\nand improve the accuracy of the model. \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q NO. 1 :\n",
    "'''\n",
    "The process of minimizing (or maximizing) any mathematical expression is called optimization.\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and \n",
    "learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function.\n",
    "\n",
    "The process of optimisation aims to lower the risk of errors or loss from these predictions, \n",
    "and improve the accuracy of the model. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c7e4ce-1ae4-48cc-8edc-eed8f9640cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Batch Gradient Descent\\nBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. \\n2. Mini-batch Gradient Descent\\nInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on the batch size\\n3. Stochastic Gradient Descent\\nInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on\\neach example (x^i,y^i). \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "'''\n",
    "1. Batch Gradient Descent\n",
    "Batch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. \n",
    "2. Mini-batch Gradient Descent\n",
    "Instead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on the batch size\n",
    "3. Stochastic Gradient Descent\n",
    "Instead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on\n",
    "each example (x^i,y^i). \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023b218a-391f-468f-8b1e-1cb9823f9618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe approaches to handle these problems:\\nI. Changing the architecture\\n\\nThis solution could be used in both the exploding and vanishing gradient problems, but requires good understanding\\nand outcomes of the change.\\nFor example, if we reduce the number of layers in our network, we give up some of our model’s complexity, \\nsince having more layers makes the networks more capable of representing complex mappings.\\n\\nII. Gradient Clipping for Exploding Gradients\\n\\nCarefully monitoring and limiting the size of the gradients whilst our model trains is yet another solution. \\nThis requires some deep knowledge around how the changes could impact the overall performance.\\n\\nIII. Careful Weight Initialization\\n\\nA more careful initialization of the model parameters for our network is a partial solution, \\nsince it does not solve the problem completely.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "The approaches to handle these problems:\n",
    "I. Changing the architecture\n",
    "\n",
    "This solution could be used in both the exploding and vanishing gradient problems, but requires good understanding\n",
    "and outcomes of the change.\n",
    "For example, if we reduce the number of layers in our network, we give up some of our model’s complexity, \n",
    "since having more layers makes the networks more capable of representing complex mappings.\n",
    "\n",
    "II. Gradient Clipping for Exploding Gradients\n",
    "\n",
    "Carefully monitoring and limiting the size of the gradients whilst our model trains is yet another solution. \n",
    "This requires some deep knowledge around how the changes could impact the overall performance.\n",
    "\n",
    "III. Careful Weight Initialization\n",
    "\n",
    "A more careful initialization of the model parameters for our network is a partial solution, \n",
    "since it does not solve the problem completely.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f376c924-6517-4e26-83e4-114fde466f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe learning rate specifies the step size towards a minimum of the loss function when following the gradient,\\nwhile the momentum weight considers previous weight changes when updating current weights.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"\n",
    "The learning rate specifies the step size towards a minimum of the loss function when following the gradient,\n",
    "while the momentum weight considers previous weight changes when updating current weights.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8a1d0-001f-40a1-a6e4-28ce6783c4a8",
   "metadata": {},
   "source": [
    "Part 2: Optimizer Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276d5420-dc04-4fc3-babf-b4be9c78ee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStochastic Gradient Descent (SGD):\\nStochastic Gradient Descent (SGD) is a variant of the Gradient Descent algorithm that is used for optimizing machine \\nlearning models. It addresses the computational inefficiency of traditional Gradient Descent methods when dealing with\\nlarge datasets in machine learning projects.\\n\\nIn SGD, instead of using the entire dataset for each iteration, only a single random training example (or a small batch) \\nis selected to calculate the gradient and update the model parameters. This random selection introduces randomness into\\nthe optimization process, hence the term “stochastic” in stochastic Gradient Descent.\\nAdvantages of Stochastic Gradient Descent  \\nSpeed: SGD is faster than other variants of Gradient Descent such as Batch Gradient Descent and Mini-Batch Gradient Descent\\nsince it uses only one example to update the parameters.\\n\\nMemory Efficiency: Since SGD updates the parameters for each training example one at a time, it is memory-efficient and \\ncan handle large datasets that cannot fit into memory.\\n\\nAvoidance of Local Minima: Due to the noisy updates in SGD, it has the ability to escape from local minima and converges \\nto a global minimum.\\n\\nDisadvantages of Stochastic Gradient Descent \\nNoisy updates: The updates in SGD are noisy and have a high variance, which can make the optimization process less stable \\nand lead to oscillations around the minimum.\\n\\nSlow Convergence: SGD may require more iterations to converge to the minimum since it updates the parameters for each training\\nexample one at a time.\\n\\nSensitivity to Learning Rate: The choice of learning rate can be critical in SGD since using a high learning rate can cause \\nthe algorithm to overshoot the minimum, while a low learning rate can make the algorithm converge slowly.\\n\\nLess Accurate: Due to the noisy updates, SGD may not converge to the exact global minimum and can result in a suboptimal \\nsolution. This can be mitigated by using techniques such as learning rate scheduling and momentum-based update.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"\n",
    "Stochastic Gradient Descent (SGD):\n",
    "Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent algorithm that is used for optimizing machine \n",
    "learning models. It addresses the computational inefficiency of traditional Gradient Descent methods when dealing with\n",
    "large datasets in machine learning projects.\n",
    "\n",
    "In SGD, instead of using the entire dataset for each iteration, only a single random training example (or a small batch) \n",
    "is selected to calculate the gradient and update the model parameters. This random selection introduces randomness into\n",
    "the optimization process, hence the term “stochastic” in stochastic Gradient Descent.\n",
    "Advantages of Stochastic Gradient Descent  \n",
    "Speed: SGD is faster than other variants of Gradient Descent such as Batch Gradient Descent and Mini-Batch Gradient Descent\n",
    "since it uses only one example to update the parameters.\n",
    "\n",
    "Memory Efficiency: Since SGD updates the parameters for each training example one at a time, it is memory-efficient and \n",
    "can handle large datasets that cannot fit into memory.\n",
    "\n",
    "Avoidance of Local Minima: Due to the noisy updates in SGD, it has the ability to escape from local minima and converges \n",
    "to a global minimum.\n",
    "\n",
    "Disadvantages of Stochastic Gradient Descent \n",
    "Noisy updates: The updates in SGD are noisy and have a high variance, which can make the optimization process less stable \n",
    "and lead to oscillations around the minimum.\n",
    "\n",
    "Slow Convergence: SGD may require more iterations to converge to the minimum since it updates the parameters for each training\n",
    "example one at a time.\n",
    "\n",
    "Sensitivity to Learning Rate: The choice of learning rate can be critical in SGD since using a high learning rate can cause \n",
    "the algorithm to overshoot the minimum, while a low learning rate can make the algorithm converge slowly.\n",
    "\n",
    "Less Accurate: Due to the noisy updates, SGD may not converge to the exact global minimum and can result in a suboptimal \n",
    "solution. This can be mitigated by using techniques such as learning rate scheduling and momentum-based update.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80fc974-303f-4990-aa38-c175baf84c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAdam uses a combination of adaptive learning rates and momentum to make adjustments to the network's parameters during\\ntraining. This helps the neural network learn faster and converge more quickly towards the optimal set of parameters that\\nminimize the cost or loss function.\\nAdvantages of Adam Optimizer\\nSimple to put into action.\\nEffective in terms of computation.\\nMemory requirements are minimal.\\nAppropriate for gradients that are very noisy or sparse.\\nIdeal for problems with a large amount of data or parameters.\\n\\nDisadvantages:\\nAdam does not converge to an optimal solution in some areas (this is the motivation for AMSGrad).\\nAdam can suffer a weight decay problem (which is addressed in AdamW).\\nRecent optimization algorithms have been proven faster and better.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "\"\"\"\n",
    "Adam uses a combination of adaptive learning rates and momentum to make adjustments to the network's parameters during\n",
    "training. This helps the neural network learn faster and converge more quickly towards the optimal set of parameters that\n",
    "minimize the cost or loss function.\n",
    "Advantages of Adam Optimizer\n",
    "Simple to put into action.\n",
    "Effective in terms of computation.\n",
    "Memory requirements are minimal.\n",
    "Appropriate for gradients that are very noisy or sparse.\n",
    "Ideal for problems with a large amount of data or parameters.\n",
    "\n",
    "Disadvantages:\n",
    "Adam does not converge to an optimal solution in some areas (this is the motivation for AMSGrad).\n",
    "Adam can suffer a weight decay problem (which is addressed in AdamW).\n",
    "Recent optimization algorithms have been proven faster and better.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e2543d-113e-419c-be05-add319dc8468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRMSprop Optimizer\\nRMSprop is a gradient-based optimization technique used in training neural networks. It was proposed by the father of \\nback-propagation, Geoffrey Hinton. Gradients of very complex functions like neural networks have a tendency to either \\nvanish or explode as the data propagates through the function (refer to vanishing gradients problem). \\nRmsprop was developed as a stochastic technique for mini-batch learning.\\n\\nRMSprop deals with the above issue by using a moving average of squared gradients to normalize the gradient. \\nThis normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding and \\nincreasing the step for small gradients to avoid vanishing.\\n\\nSimply put, RMSprop uses an adaptive learning rate instead of treating the learning rate as a hyperparameter. \\nThis means that the learning rate changes over time.\\n\\nThere are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its\\nparameter updates using momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running \\naverage of the first and second moment of the gradient.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "\"\"\"\n",
    "RMSprop Optimizer\n",
    "RMSprop is a gradient-based optimization technique used in training neural networks. It was proposed by the father of \n",
    "back-propagation, Geoffrey Hinton. Gradients of very complex functions like neural networks have a tendency to either \n",
    "vanish or explode as the data propagates through the function (refer to vanishing gradients problem). \n",
    "Rmsprop was developed as a stochastic technique for mini-batch learning.\n",
    "\n",
    "RMSprop deals with the above issue by using a moving average of squared gradients to normalize the gradient. \n",
    "This normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding and \n",
    "increasing the step for small gradients to avoid vanishing.\n",
    "\n",
    "Simply put, RMSprop uses an adaptive learning rate instead of treating the learning rate as a hyperparameter. \n",
    "This means that the learning rate changes over time.\n",
    "\n",
    "There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its\n",
    "parameter updates using momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running \n",
    "average of the first and second moment of the gradient.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e0b1b2-b2db-4a26-b50b-a293d955727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (2.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db3cfb8-638e-40ad-b3ed-73a29d693377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.13)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.21.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 keras-2.13.1 libclang-16.0.0 markdown-3.4.3 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326d972-5cea-422e-a81b-ad5aeeee6aef",
   "metadata": {},
   "source": [
    "Part 3: Applying Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f193a23-822f-4455-836b-b95808f186f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q No. 8 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825d3f5e-51bc-485f-9ddc-19d6f5185792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75c6044-9619-49ef-8d39-05447b74af12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Churn_Modelling.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9e4341-a0fc-4c87-9445-c639332fbc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,3:13]\n",
    "y=df.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775a9265-b725-4131-a414-d2a3290c82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "geography=pd.get_dummies(X[\"Geography\"],drop_first=True)\n",
    "gender=pd.get_dummies(X[\"Gender\"],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bbeed6b-3c9d-4ba3-802a-0dd2df6aea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.concat([X,geography,gender],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d56db64-075c-4c36-b763-69391b6389ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop(['Geography'\t,\"Gender\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce9fc80-5164-4a07-9de4-e7f3eceeabb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0          619   42       2       0.00              1          1   \n",
       "1          608   41       1   83807.86              1          0   \n",
       "2          502   42       8  159660.80              3          1   \n",
       "3          699   39       1       0.00              2          0   \n",
       "4          850   43       2  125510.82              1          1   \n",
       "\n",
       "   IsActiveMember  EstimatedSalary  Germany  Spain  Male  \n",
       "0               1        101348.88        0      0     0  \n",
       "1               1        112542.58        0      1     0  \n",
       "2               0        113931.57        0      0     0  \n",
       "3               0         93826.63        0      0     0  \n",
       "4               1         79084.10        0      1     0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c717375-b360-4228-8dc2-53458bc1f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5a3d3c6-8ad6-4523-8624-34156db684f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7545050-395f-4bd0-bad8-6bcdfb0b75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import PReLU,ReLU,ELU,LeakyReLU\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e660fa54-560e-4392-9ece-4c3ba00c294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f8db6e3-1c9d-4560-b95d-c962bf09b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and first hidden layer\n",
    "classifier.add(Dense(units=6,kernel_initializer='he_uniform',activation='relu',input_dim=11))\n",
    "# hidden layer2 \n",
    "classifier.add(Dense(units=6,kernel_initializer='he_uniform',activation='relu'))\n",
    "#output layer\n",
    "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "# Compile\n",
    "classifier.compile(optimizer=\"Adamax\",loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5be008cc-22c2-4e76-9ad4-02cc8017063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1798: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 2s 2ms/step - loss: 0.7587 - accuracy: 0.4664 - val_loss: 0.6212 - val_accuracy: 0.6826\n",
      "Epoch 2/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.5695 - accuracy: 0.7529 - val_loss: 0.5338 - val_accuracy: 0.7839\n",
      "Epoch 3/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.5142 - accuracy: 0.7899 - val_loss: 0.5017 - val_accuracy: 0.7889\n",
      "Epoch 4/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4925 - accuracy: 0.7926 - val_loss: 0.4888 - val_accuracy: 0.7907\n",
      "Epoch 5/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4814 - accuracy: 0.7926 - val_loss: 0.4795 - val_accuracy: 0.7907\n",
      "Epoch 6/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4725 - accuracy: 0.7926 - val_loss: 0.4718 - val_accuracy: 0.7911\n",
      "Epoch 7/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4650 - accuracy: 0.7926 - val_loss: 0.4650 - val_accuracy: 0.7916\n",
      "Epoch 8/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4589 - accuracy: 0.7921 - val_loss: 0.4590 - val_accuracy: 0.7929\n",
      "Epoch 9/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4536 - accuracy: 0.7934 - val_loss: 0.4541 - val_accuracy: 0.7966\n",
      "Epoch 10/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4488 - accuracy: 0.7912 - val_loss: 0.4497 - val_accuracy: 0.7979\n",
      "Epoch 11/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4444 - accuracy: 0.7932 - val_loss: 0.4458 - val_accuracy: 0.7993\n",
      "Epoch 12/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4408 - accuracy: 0.7950 - val_loss: 0.4427 - val_accuracy: 0.8020\n",
      "Epoch 13/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4378 - accuracy: 0.7977 - val_loss: 0.4403 - val_accuracy: 0.8020\n",
      "Epoch 14/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4347 - accuracy: 0.8001 - val_loss: 0.4377 - val_accuracy: 0.8042\n",
      "Epoch 15/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4315 - accuracy: 0.8024 - val_loss: 0.4356 - val_accuracy: 0.8074\n",
      "Epoch 16/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4286 - accuracy: 0.8030 - val_loss: 0.4334 - val_accuracy: 0.8088\n",
      "Epoch 17/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4255 - accuracy: 0.8057 - val_loss: 0.4309 - val_accuracy: 0.8097\n",
      "Epoch 18/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4223 - accuracy: 0.8079 - val_loss: 0.4285 - val_accuracy: 0.8115\n",
      "Epoch 19/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4192 - accuracy: 0.8111 - val_loss: 0.4262 - val_accuracy: 0.8142\n",
      "Epoch 20/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4158 - accuracy: 0.8124 - val_loss: 0.4233 - val_accuracy: 0.8151\n",
      "Epoch 21/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4123 - accuracy: 0.8177 - val_loss: 0.4210 - val_accuracy: 0.8142\n",
      "Epoch 22/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4089 - accuracy: 0.8204 - val_loss: 0.4186 - val_accuracy: 0.8156\n",
      "Epoch 23/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4054 - accuracy: 0.8233 - val_loss: 0.4159 - val_accuracy: 0.8187\n",
      "Epoch 24/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.4018 - accuracy: 0.8271 - val_loss: 0.4132 - val_accuracy: 0.8214\n",
      "Epoch 25/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3982 - accuracy: 0.8278 - val_loss: 0.4100 - val_accuracy: 0.8250\n",
      "Epoch 26/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3947 - accuracy: 0.8304 - val_loss: 0.4073 - val_accuracy: 0.8273\n",
      "Epoch 27/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3913 - accuracy: 0.8329 - val_loss: 0.4046 - val_accuracy: 0.8282\n",
      "Epoch 28/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3884 - accuracy: 0.8356 - val_loss: 0.4026 - val_accuracy: 0.8300\n",
      "Epoch 29/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3855 - accuracy: 0.8387 - val_loss: 0.4001 - val_accuracy: 0.8314\n",
      "Epoch 30/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3826 - accuracy: 0.8400 - val_loss: 0.3977 - val_accuracy: 0.8336\n",
      "Epoch 31/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3801 - accuracy: 0.8416 - val_loss: 0.3960 - val_accuracy: 0.8336\n",
      "Epoch 32/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3778 - accuracy: 0.8449 - val_loss: 0.3940 - val_accuracy: 0.8341\n",
      "Epoch 33/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3756 - accuracy: 0.8454 - val_loss: 0.3923 - val_accuracy: 0.8363\n",
      "Epoch 34/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3733 - accuracy: 0.8471 - val_loss: 0.3907 - val_accuracy: 0.8359\n",
      "Epoch 35/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3714 - accuracy: 0.8469 - val_loss: 0.3891 - val_accuracy: 0.8386\n",
      "Epoch 36/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3698 - accuracy: 0.8476 - val_loss: 0.3878 - val_accuracy: 0.8368\n",
      "Epoch 37/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3682 - accuracy: 0.8485 - val_loss: 0.3869 - val_accuracy: 0.8377\n",
      "Epoch 38/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3667 - accuracy: 0.8489 - val_loss: 0.3858 - val_accuracy: 0.8404\n",
      "Epoch 39/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3655 - accuracy: 0.8485 - val_loss: 0.3847 - val_accuracy: 0.8382\n",
      "Epoch 40/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3642 - accuracy: 0.8496 - val_loss: 0.3836 - val_accuracy: 0.8400\n",
      "Epoch 41/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3629 - accuracy: 0.8507 - val_loss: 0.3827 - val_accuracy: 0.8386\n",
      "Epoch 42/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3619 - accuracy: 0.8512 - val_loss: 0.3816 - val_accuracy: 0.8386\n",
      "Epoch 43/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3607 - accuracy: 0.8520 - val_loss: 0.3807 - val_accuracy: 0.8413\n",
      "Epoch 44/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3598 - accuracy: 0.8523 - val_loss: 0.3799 - val_accuracy: 0.8395\n",
      "Epoch 45/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3586 - accuracy: 0.8527 - val_loss: 0.3791 - val_accuracy: 0.8422\n",
      "Epoch 46/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3577 - accuracy: 0.8529 - val_loss: 0.3782 - val_accuracy: 0.8400\n",
      "Epoch 47/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3568 - accuracy: 0.8529 - val_loss: 0.3774 - val_accuracy: 0.8427\n",
      "Epoch 48/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3558 - accuracy: 0.8514 - val_loss: 0.3763 - val_accuracy: 0.8427\n",
      "Epoch 49/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3550 - accuracy: 0.8532 - val_loss: 0.3756 - val_accuracy: 0.8431\n",
      "Epoch 50/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3542 - accuracy: 0.8534 - val_loss: 0.3748 - val_accuracy: 0.8431\n",
      "Epoch 51/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3535 - accuracy: 0.8543 - val_loss: 0.3741 - val_accuracy: 0.8454\n",
      "Epoch 52/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3528 - accuracy: 0.8536 - val_loss: 0.3734 - val_accuracy: 0.8454\n",
      "Epoch 53/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3524 - accuracy: 0.8552 - val_loss: 0.3728 - val_accuracy: 0.8463\n",
      "Epoch 54/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3517 - accuracy: 0.8561 - val_loss: 0.3724 - val_accuracy: 0.8467\n",
      "Epoch 55/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3512 - accuracy: 0.8554 - val_loss: 0.3719 - val_accuracy: 0.8463\n",
      "Epoch 56/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3507 - accuracy: 0.8538 - val_loss: 0.3714 - val_accuracy: 0.8467\n",
      "Epoch 57/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3503 - accuracy: 0.8529 - val_loss: 0.3709 - val_accuracy: 0.8481\n",
      "Epoch 58/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3497 - accuracy: 0.8545 - val_loss: 0.3702 - val_accuracy: 0.8486\n",
      "Epoch 59/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3492 - accuracy: 0.8552 - val_loss: 0.3697 - val_accuracy: 0.8481\n",
      "Epoch 60/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3489 - accuracy: 0.8549 - val_loss: 0.3691 - val_accuracy: 0.8472\n",
      "Epoch 61/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3485 - accuracy: 0.8556 - val_loss: 0.3688 - val_accuracy: 0.8476\n",
      "Epoch 62/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3481 - accuracy: 0.8558 - val_loss: 0.3684 - val_accuracy: 0.8490\n",
      "Epoch 63/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3478 - accuracy: 0.8563 - val_loss: 0.3683 - val_accuracy: 0.8499\n",
      "Epoch 64/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8565 - val_loss: 0.3679 - val_accuracy: 0.8495\n",
      "Epoch 65/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3471 - accuracy: 0.8561 - val_loss: 0.3675 - val_accuracy: 0.8490\n",
      "Epoch 66/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3468 - accuracy: 0.8570 - val_loss: 0.3672 - val_accuracy: 0.8495\n",
      "Epoch 67/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3465 - accuracy: 0.8563 - val_loss: 0.3670 - val_accuracy: 0.8504\n",
      "Epoch 68/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3461 - accuracy: 0.8574 - val_loss: 0.3666 - val_accuracy: 0.8495\n",
      "Epoch 69/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3461 - accuracy: 0.8565 - val_loss: 0.3664 - val_accuracy: 0.8495\n",
      "Epoch 70/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3458 - accuracy: 0.8561 - val_loss: 0.3662 - val_accuracy: 0.8495\n",
      "Epoch 71/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3454 - accuracy: 0.8565 - val_loss: 0.3658 - val_accuracy: 0.8499\n",
      "Epoch 72/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8570 - val_loss: 0.3657 - val_accuracy: 0.8499\n",
      "Epoch 73/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8576 - val_loss: 0.3654 - val_accuracy: 0.8495\n",
      "Epoch 74/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3447 - accuracy: 0.8583 - val_loss: 0.3651 - val_accuracy: 0.8495\n",
      "Epoch 75/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3446 - accuracy: 0.8576 - val_loss: 0.3649 - val_accuracy: 0.8504\n",
      "Epoch 76/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3444 - accuracy: 0.8592 - val_loss: 0.3646 - val_accuracy: 0.8508\n",
      "Epoch 77/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3441 - accuracy: 0.8583 - val_loss: 0.3644 - val_accuracy: 0.8499\n",
      "Epoch 78/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3439 - accuracy: 0.8583 - val_loss: 0.3643 - val_accuracy: 0.8490\n",
      "Epoch 79/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3436 - accuracy: 0.8578 - val_loss: 0.3642 - val_accuracy: 0.8499\n",
      "Epoch 80/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3435 - accuracy: 0.8587 - val_loss: 0.3639 - val_accuracy: 0.8504\n",
      "Epoch 81/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3434 - accuracy: 0.8585 - val_loss: 0.3637 - val_accuracy: 0.8504\n",
      "Epoch 82/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3432 - accuracy: 0.8592 - val_loss: 0.3635 - val_accuracy: 0.8495\n",
      "Epoch 83/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3430 - accuracy: 0.8583 - val_loss: 0.3632 - val_accuracy: 0.8504\n",
      "Epoch 84/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3428 - accuracy: 0.8583 - val_loss: 0.3631 - val_accuracy: 0.8504\n",
      "Epoch 85/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3427 - accuracy: 0.8581 - val_loss: 0.3629 - val_accuracy: 0.8517\n",
      "Epoch 86/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3425 - accuracy: 0.8596 - val_loss: 0.3627 - val_accuracy: 0.8508\n",
      "Epoch 87/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3424 - accuracy: 0.8592 - val_loss: 0.3626 - val_accuracy: 0.8517\n",
      "Epoch 88/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3422 - accuracy: 0.8596 - val_loss: 0.3624 - val_accuracy: 0.8508\n",
      "Epoch 89/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8585 - val_loss: 0.3624 - val_accuracy: 0.8508\n",
      "Epoch 90/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3420 - accuracy: 0.8576 - val_loss: 0.3624 - val_accuracy: 0.8535\n",
      "Epoch 91/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3419 - accuracy: 0.8603 - val_loss: 0.3622 - val_accuracy: 0.8517\n",
      "Epoch 92/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3417 - accuracy: 0.8590 - val_loss: 0.3620 - val_accuracy: 0.8535\n",
      "Epoch 93/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3416 - accuracy: 0.8596 - val_loss: 0.3619 - val_accuracy: 0.8526\n",
      "Epoch 94/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3415 - accuracy: 0.8592 - val_loss: 0.3619 - val_accuracy: 0.8513\n",
      "Epoch 95/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3414 - accuracy: 0.8598 - val_loss: 0.3617 - val_accuracy: 0.8517\n",
      "Epoch 96/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3412 - accuracy: 0.8605 - val_loss: 0.3616 - val_accuracy: 0.8522\n",
      "Epoch 97/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3411 - accuracy: 0.8585 - val_loss: 0.3612 - val_accuracy: 0.8522\n",
      "Epoch 98/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3411 - accuracy: 0.8596 - val_loss: 0.3613 - val_accuracy: 0.8531\n",
      "Epoch 99/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3410 - accuracy: 0.8585 - val_loss: 0.3611 - val_accuracy: 0.8535\n",
      "Epoch 100/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3407 - accuracy: 0.8578 - val_loss: 0.3610 - val_accuracy: 0.8535\n"
     ]
    }
   ],
   "source": [
    "# fit ANN to the model\n",
    "model_history=classifier.fit(X_train,y_train,batch_size=10,epochs=100,validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a15a1ad-4c70-4509-9d2f-8bc960e09572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 966us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "y_pred=(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d879711a-2153-4a7e-b7c9-fc9f49b90526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3354 - accuracy: 0.8601 - val_loss: 0.3551 - val_accuracy: 0.8522\n",
      "Epoch 2/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3347 - accuracy: 0.8612 - val_loss: 0.3565 - val_accuracy: 0.8495\n",
      "Epoch 3/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3351 - accuracy: 0.8612 - val_loss: 0.3533 - val_accuracy: 0.8513\n",
      "Epoch 4/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3351 - accuracy: 0.8581 - val_loss: 0.3548 - val_accuracy: 0.8558\n",
      "Epoch 5/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3348 - accuracy: 0.8598 - val_loss: 0.3554 - val_accuracy: 0.8481\n",
      "Epoch 6/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8607 - val_loss: 0.3562 - val_accuracy: 0.8486\n",
      "Epoch 7/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3352 - accuracy: 0.8590 - val_loss: 0.3550 - val_accuracy: 0.8495\n",
      "Epoch 8/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8594 - val_loss: 0.3543 - val_accuracy: 0.8490\n",
      "Epoch 9/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8605 - val_loss: 0.3535 - val_accuracy: 0.8522\n",
      "Epoch 10/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8614 - val_loss: 0.3534 - val_accuracy: 0.8522\n",
      "Epoch 11/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8585 - val_loss: 0.3540 - val_accuracy: 0.8495\n",
      "Epoch 12/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8603 - val_loss: 0.3540 - val_accuracy: 0.8508\n",
      "Epoch 13/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3335 - accuracy: 0.8598 - val_loss: 0.3545 - val_accuracy: 0.8495\n",
      "Epoch 14/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3339 - accuracy: 0.8610 - val_loss: 0.3534 - val_accuracy: 0.8540\n",
      "Epoch 15/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3341 - accuracy: 0.8592 - val_loss: 0.3538 - val_accuracy: 0.8495\n",
      "Epoch 16/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8587 - val_loss: 0.3558 - val_accuracy: 0.8526\n",
      "Epoch 17/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8587 - val_loss: 0.3554 - val_accuracy: 0.8517\n",
      "Epoch 18/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3339 - accuracy: 0.8587 - val_loss: 0.3534 - val_accuracy: 0.8531\n",
      "Epoch 19/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8610 - val_loss: 0.3531 - val_accuracy: 0.8526\n",
      "Epoch 20/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8601 - val_loss: 0.3529 - val_accuracy: 0.8508\n",
      "Epoch 21/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8614 - val_loss: 0.3534 - val_accuracy: 0.8535\n",
      "Epoch 22/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8603 - val_loss: 0.3535 - val_accuracy: 0.8504\n",
      "Epoch 23/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8601 - val_loss: 0.3533 - val_accuracy: 0.8544\n",
      "Epoch 24/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8601 - val_loss: 0.3540 - val_accuracy: 0.8531\n",
      "Epoch 25/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3334 - accuracy: 0.8607 - val_loss: 0.3545 - val_accuracy: 0.8522\n",
      "Epoch 26/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3333 - accuracy: 0.8619 - val_loss: 0.3539 - val_accuracy: 0.8580\n",
      "Epoch 27/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8596 - val_loss: 0.3566 - val_accuracy: 0.8481\n",
      "Epoch 28/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8603 - val_loss: 0.3554 - val_accuracy: 0.8495\n",
      "Epoch 29/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3332 - accuracy: 0.8578 - val_loss: 0.3547 - val_accuracy: 0.8513\n",
      "Epoch 30/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8614 - val_loss: 0.3536 - val_accuracy: 0.8522\n",
      "Epoch 31/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8585 - val_loss: 0.3542 - val_accuracy: 0.8522\n",
      "Epoch 32/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8619 - val_loss: 0.3567 - val_accuracy: 0.8549\n",
      "Epoch 33/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3334 - accuracy: 0.8603 - val_loss: 0.3543 - val_accuracy: 0.8504\n",
      "Epoch 34/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8610 - val_loss: 0.3548 - val_accuracy: 0.8504\n",
      "Epoch 35/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3335 - accuracy: 0.8605 - val_loss: 0.3537 - val_accuracy: 0.8535\n",
      "Epoch 36/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8583 - val_loss: 0.3556 - val_accuracy: 0.8513\n",
      "Epoch 37/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8619 - val_loss: 0.3553 - val_accuracy: 0.8531\n",
      "Epoch 38/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8574 - val_loss: 0.3539 - val_accuracy: 0.8499\n",
      "Epoch 39/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8605 - val_loss: 0.3539 - val_accuracy: 0.8508\n",
      "Epoch 40/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8603 - val_loss: 0.3535 - val_accuracy: 0.8540\n",
      "Epoch 41/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8598 - val_loss: 0.3556 - val_accuracy: 0.8562\n",
      "Epoch 42/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8607 - val_loss: 0.3536 - val_accuracy: 0.8513\n",
      "Epoch 43/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8607 - val_loss: 0.3555 - val_accuracy: 0.8513\n",
      "Epoch 44/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3327 - accuracy: 0.8621 - val_loss: 0.3555 - val_accuracy: 0.8481\n",
      "Epoch 45/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3327 - accuracy: 0.8598 - val_loss: 0.3540 - val_accuracy: 0.8567\n",
      "Epoch 46/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3327 - accuracy: 0.8619 - val_loss: 0.3553 - val_accuracy: 0.8486\n",
      "Epoch 47/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8610 - val_loss: 0.3557 - val_accuracy: 0.8499\n",
      "Epoch 48/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8601 - val_loss: 0.3543 - val_accuracy: 0.8540\n",
      "Epoch 49/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8581 - val_loss: 0.3541 - val_accuracy: 0.8508\n",
      "Epoch 50/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8605 - val_loss: 0.3549 - val_accuracy: 0.8472\n",
      "Epoch 51/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8603 - val_loss: 0.3540 - val_accuracy: 0.8549\n",
      "Epoch 52/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8601 - val_loss: 0.3545 - val_accuracy: 0.8513\n",
      "Epoch 53/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8592 - val_loss: 0.3542 - val_accuracy: 0.8517\n",
      "Epoch 54/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8594 - val_loss: 0.3544 - val_accuracy: 0.8540\n",
      "Epoch 55/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8576 - val_loss: 0.3561 - val_accuracy: 0.8472\n",
      "Epoch 56/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8627 - val_loss: 0.3566 - val_accuracy: 0.8476\n",
      "Epoch 57/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8630 - val_loss: 0.3554 - val_accuracy: 0.8571\n",
      "Epoch 58/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8572 - val_loss: 0.3562 - val_accuracy: 0.8490\n",
      "Epoch 59/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8596 - val_loss: 0.3543 - val_accuracy: 0.8495\n",
      "Epoch 60/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8619 - val_loss: 0.3555 - val_accuracy: 0.8486\n",
      "Epoch 61/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3323 - accuracy: 0.8587 - val_loss: 0.3543 - val_accuracy: 0.8495\n",
      "Epoch 62/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8610 - val_loss: 0.3566 - val_accuracy: 0.8495\n",
      "Epoch 63/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3320 - accuracy: 0.8603 - val_loss: 0.3559 - val_accuracy: 0.8531\n",
      "Epoch 64/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8590 - val_loss: 0.3545 - val_accuracy: 0.8508\n",
      "Epoch 65/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8587 - val_loss: 0.3565 - val_accuracy: 0.8558\n",
      "Epoch 66/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8610 - val_loss: 0.3547 - val_accuracy: 0.8517\n",
      "Epoch 67/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8610 - val_loss: 0.3557 - val_accuracy: 0.8522\n",
      "Epoch 68/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3320 - accuracy: 0.8619 - val_loss: 0.3564 - val_accuracy: 0.8486\n",
      "Epoch 69/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8610 - val_loss: 0.3572 - val_accuracy: 0.8490\n",
      "Epoch 70/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8610 - val_loss: 0.3564 - val_accuracy: 0.8490\n",
      "Epoch 71/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8592 - val_loss: 0.3559 - val_accuracy: 0.8499\n",
      "Epoch 72/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8603 - val_loss: 0.3551 - val_accuracy: 0.8535\n",
      "Epoch 73/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8612 - val_loss: 0.3555 - val_accuracy: 0.8513\n",
      "Epoch 74/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8601 - val_loss: 0.3561 - val_accuracy: 0.8486\n",
      "Epoch 75/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8603 - val_loss: 0.3546 - val_accuracy: 0.8549\n",
      "Epoch 76/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8612 - val_loss: 0.3548 - val_accuracy: 0.8499\n",
      "Epoch 77/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8627 - val_loss: 0.3547 - val_accuracy: 0.8531\n",
      "Epoch 78/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8625 - val_loss: 0.3554 - val_accuracy: 0.8499\n",
      "Epoch 79/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8607 - val_loss: 0.3549 - val_accuracy: 0.8499\n",
      "Epoch 80/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8594 - val_loss: 0.3550 - val_accuracy: 0.8540\n",
      "Epoch 81/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8616 - val_loss: 0.3570 - val_accuracy: 0.8504\n",
      "Epoch 82/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8603 - val_loss: 0.3553 - val_accuracy: 0.8531\n",
      "Epoch 83/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8601 - val_loss: 0.3557 - val_accuracy: 0.8508\n",
      "Epoch 84/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8587 - val_loss: 0.3555 - val_accuracy: 0.8499\n",
      "Epoch 85/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8590 - val_loss: 0.3565 - val_accuracy: 0.8499\n",
      "Epoch 86/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8605 - val_loss: 0.3558 - val_accuracy: 0.8495\n",
      "Epoch 87/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8627 - val_loss: 0.3548 - val_accuracy: 0.8522\n",
      "Epoch 88/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8590 - val_loss: 0.3542 - val_accuracy: 0.8486\n",
      "Epoch 89/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8607 - val_loss: 0.3559 - val_accuracy: 0.8544\n",
      "Epoch 90/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8634 - val_loss: 0.3541 - val_accuracy: 0.8513\n",
      "Epoch 91/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8605 - val_loss: 0.3543 - val_accuracy: 0.8535\n",
      "Epoch 92/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8619 - val_loss: 0.3564 - val_accuracy: 0.8486\n",
      "Epoch 93/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8619 - val_loss: 0.3540 - val_accuracy: 0.8513\n",
      "Epoch 94/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8592 - val_loss: 0.3543 - val_accuracy: 0.8508\n",
      "Epoch 95/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8598 - val_loss: 0.3538 - val_accuracy: 0.8508\n",
      "Epoch 96/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8601 - val_loss: 0.3551 - val_accuracy: 0.8490\n",
      "Epoch 97/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8616 - val_loss: 0.3545 - val_accuracy: 0.8504\n",
      "Epoch 98/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3301 - accuracy: 0.8616 - val_loss: 0.3543 - val_accuracy: 0.8486\n",
      "Epoch 99/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8610 - val_loss: 0.3539 - val_accuracy: 0.8495\n",
      "Epoch 100/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8612 - val_loss: 0.3544 - val_accuracy: 0.8549\n"
     ]
    }
   ],
   "source": [
    "classifier.compile(optimizer=\"SGD\",loss='binary_crossentropy',metrics=['accuracy'])\n",
    "# fit ANN to the model\n",
    "model_history=classifier.fit(X_train,y_train,batch_size=10,epochs=100,validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e450f87-1e57-49c5-ad83-e411823fe884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "449/449 [==============================] - 2s 2ms/step - loss: 0.3303 - accuracy: 0.8612 - val_loss: 0.3542 - val_accuracy: 0.8486\n",
      "Epoch 2/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8625 - val_loss: 0.3539 - val_accuracy: 0.8499\n",
      "Epoch 3/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8623 - val_loss: 0.3537 - val_accuracy: 0.8517\n",
      "Epoch 4/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8625 - val_loss: 0.3547 - val_accuracy: 0.8499\n",
      "Epoch 5/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8623 - val_loss: 0.3541 - val_accuracy: 0.8508\n",
      "Epoch 6/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8636 - val_loss: 0.3537 - val_accuracy: 0.8499\n",
      "Epoch 7/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8623 - val_loss: 0.3551 - val_accuracy: 0.8486\n",
      "Epoch 8/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8603 - val_loss: 0.3554 - val_accuracy: 0.8517\n",
      "Epoch 9/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8610 - val_loss: 0.3549 - val_accuracy: 0.8504\n",
      "Epoch 10/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 0.8605 - val_loss: 0.3557 - val_accuracy: 0.8504\n",
      "Epoch 11/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8619 - val_loss: 0.3562 - val_accuracy: 0.8504\n",
      "Epoch 12/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8616 - val_loss: 0.3573 - val_accuracy: 0.8476\n",
      "Epoch 13/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3313 - accuracy: 0.8610 - val_loss: 0.3548 - val_accuracy: 0.8504\n",
      "Epoch 14/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8596 - val_loss: 0.3589 - val_accuracy: 0.8463\n",
      "Epoch 15/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8621 - val_loss: 0.3566 - val_accuracy: 0.8490\n",
      "Epoch 16/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8636 - val_loss: 0.3567 - val_accuracy: 0.8495\n",
      "Epoch 17/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8614 - val_loss: 0.3559 - val_accuracy: 0.8467\n",
      "Epoch 18/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8607 - val_loss: 0.3558 - val_accuracy: 0.8486\n",
      "Epoch 19/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8610 - val_loss: 0.3565 - val_accuracy: 0.8490\n",
      "Epoch 20/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8594 - val_loss: 0.3559 - val_accuracy: 0.8490\n",
      "Epoch 21/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8612 - val_loss: 0.3548 - val_accuracy: 0.8504\n",
      "Epoch 22/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8605 - val_loss: 0.3549 - val_accuracy: 0.8495\n",
      "Epoch 23/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8601 - val_loss: 0.3554 - val_accuracy: 0.8499\n",
      "Epoch 24/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8614 - val_loss: 0.3573 - val_accuracy: 0.8454\n",
      "Epoch 25/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8623 - val_loss: 0.3572 - val_accuracy: 0.8476\n",
      "Epoch 26/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8598 - val_loss: 0.3562 - val_accuracy: 0.8490\n",
      "Epoch 27/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8614 - val_loss: 0.3560 - val_accuracy: 0.8476\n",
      "Epoch 28/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3323 - accuracy: 0.8616 - val_loss: 0.3556 - val_accuracy: 0.8495\n",
      "Epoch 29/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8627 - val_loss: 0.3561 - val_accuracy: 0.8472\n",
      "Epoch 30/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8623 - val_loss: 0.3561 - val_accuracy: 0.8476\n",
      "Epoch 31/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8607 - val_loss: 0.3551 - val_accuracy: 0.8467\n",
      "Epoch 32/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3320 - accuracy: 0.8612 - val_loss: 0.3555 - val_accuracy: 0.8458\n",
      "Epoch 33/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8616 - val_loss: 0.3565 - val_accuracy: 0.8467\n",
      "Epoch 34/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8594 - val_loss: 0.3559 - val_accuracy: 0.8490\n",
      "Epoch 35/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8605 - val_loss: 0.3576 - val_accuracy: 0.8486\n",
      "Epoch 36/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8605 - val_loss: 0.3587 - val_accuracy: 0.8445\n",
      "Epoch 37/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8614 - val_loss: 0.3562 - val_accuracy: 0.8472\n",
      "Epoch 38/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8578 - val_loss: 0.3567 - val_accuracy: 0.8463\n",
      "Epoch 39/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8627 - val_loss: 0.3580 - val_accuracy: 0.8476\n",
      "Epoch 40/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8614 - val_loss: 0.3561 - val_accuracy: 0.8467\n",
      "Epoch 41/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8621 - val_loss: 0.3578 - val_accuracy: 0.8463\n",
      "Epoch 42/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8610 - val_loss: 0.3567 - val_accuracy: 0.8481\n",
      "Epoch 43/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3323 - accuracy: 0.8612 - val_loss: 0.3565 - val_accuracy: 0.8476\n",
      "Epoch 44/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8619 - val_loss: 0.3564 - val_accuracy: 0.8472\n",
      "Epoch 45/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8596 - val_loss: 0.3562 - val_accuracy: 0.8495\n",
      "Epoch 46/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8621 - val_loss: 0.3556 - val_accuracy: 0.8495\n",
      "Epoch 47/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3320 - accuracy: 0.8625 - val_loss: 0.3562 - val_accuracy: 0.8504\n",
      "Epoch 48/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8621 - val_loss: 0.3572 - val_accuracy: 0.8495\n",
      "Epoch 49/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8632 - val_loss: 0.3580 - val_accuracy: 0.8486\n",
      "Epoch 50/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8610 - val_loss: 0.3576 - val_accuracy: 0.8472\n",
      "Epoch 51/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3320 - accuracy: 0.8605 - val_loss: 0.3563 - val_accuracy: 0.8508\n",
      "Epoch 52/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8616 - val_loss: 0.3582 - val_accuracy: 0.8476\n",
      "Epoch 53/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8621 - val_loss: 0.3569 - val_accuracy: 0.8495\n",
      "Epoch 54/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3313 - accuracy: 0.8607 - val_loss: 0.3576 - val_accuracy: 0.8504\n",
      "Epoch 55/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8621 - val_loss: 0.3575 - val_accuracy: 0.8476\n",
      "Epoch 56/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8605 - val_loss: 0.3564 - val_accuracy: 0.8486\n",
      "Epoch 57/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8592 - val_loss: 0.3558 - val_accuracy: 0.8490\n",
      "Epoch 58/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8623 - val_loss: 0.3582 - val_accuracy: 0.8467\n",
      "Epoch 59/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8632 - val_loss: 0.3584 - val_accuracy: 0.8463\n",
      "Epoch 60/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3301 - accuracy: 0.8607 - val_loss: 0.3576 - val_accuracy: 0.8476\n",
      "Epoch 61/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8625 - val_loss: 0.3579 - val_accuracy: 0.8499\n",
      "Epoch 62/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8616 - val_loss: 0.3575 - val_accuracy: 0.8481\n",
      "Epoch 63/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8610 - val_loss: 0.3575 - val_accuracy: 0.8486\n",
      "Epoch 64/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8598 - val_loss: 0.3565 - val_accuracy: 0.8513\n",
      "Epoch 65/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8621 - val_loss: 0.3554 - val_accuracy: 0.8522\n",
      "Epoch 66/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8623 - val_loss: 0.3562 - val_accuracy: 0.8517\n",
      "Epoch 67/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8610 - val_loss: 0.3561 - val_accuracy: 0.8508\n",
      "Epoch 68/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8596 - val_loss: 0.3567 - val_accuracy: 0.8508\n",
      "Epoch 69/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3298 - accuracy: 0.8627 - val_loss: 0.3580 - val_accuracy: 0.8508\n",
      "Epoch 70/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8625 - val_loss: 0.3555 - val_accuracy: 0.8517\n",
      "Epoch 71/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3300 - accuracy: 0.8634 - val_loss: 0.3565 - val_accuracy: 0.8495\n",
      "Epoch 72/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8632 - val_loss: 0.3544 - val_accuracy: 0.8517\n",
      "Epoch 73/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3297 - accuracy: 0.8616 - val_loss: 0.3549 - val_accuracy: 0.8526\n",
      "Epoch 74/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3298 - accuracy: 0.8634 - val_loss: 0.3553 - val_accuracy: 0.8495\n",
      "Epoch 75/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8612 - val_loss: 0.3556 - val_accuracy: 0.8526\n",
      "Epoch 76/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3292 - accuracy: 0.8610 - val_loss: 0.3580 - val_accuracy: 0.8540\n",
      "Epoch 77/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8656 - val_loss: 0.3555 - val_accuracy: 0.8526\n",
      "Epoch 78/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8668 - val_loss: 0.3531 - val_accuracy: 0.8544\n",
      "Epoch 79/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8634 - val_loss: 0.3543 - val_accuracy: 0.8549\n",
      "Epoch 80/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3289 - accuracy: 0.8650 - val_loss: 0.3540 - val_accuracy: 0.8553\n",
      "Epoch 81/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3286 - accuracy: 0.8630 - val_loss: 0.3556 - val_accuracy: 0.8517\n",
      "Epoch 82/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3286 - accuracy: 0.8663 - val_loss: 0.3550 - val_accuracy: 0.8540\n",
      "Epoch 83/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3283 - accuracy: 0.8643 - val_loss: 0.3539 - val_accuracy: 0.8567\n",
      "Epoch 84/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3285 - accuracy: 0.8645 - val_loss: 0.3550 - val_accuracy: 0.8544\n",
      "Epoch 85/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3280 - accuracy: 0.8663 - val_loss: 0.3544 - val_accuracy: 0.8540\n",
      "Epoch 86/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3286 - accuracy: 0.8654 - val_loss: 0.3535 - val_accuracy: 0.8553\n",
      "Epoch 87/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3278 - accuracy: 0.8670 - val_loss: 0.3561 - val_accuracy: 0.8504\n",
      "Epoch 88/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3276 - accuracy: 0.8654 - val_loss: 0.3538 - val_accuracy: 0.8553\n",
      "Epoch 89/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3286 - accuracy: 0.8699 - val_loss: 0.3533 - val_accuracy: 0.8531\n",
      "Epoch 90/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3289 - accuracy: 0.8672 - val_loss: 0.3528 - val_accuracy: 0.8535\n",
      "Epoch 91/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3283 - accuracy: 0.8654 - val_loss: 0.3527 - val_accuracy: 0.8558\n",
      "Epoch 92/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3285 - accuracy: 0.8654 - val_loss: 0.3529 - val_accuracy: 0.8535\n",
      "Epoch 93/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8650 - val_loss: 0.3531 - val_accuracy: 0.8567\n",
      "Epoch 94/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3280 - accuracy: 0.8656 - val_loss: 0.3529 - val_accuracy: 0.8580\n",
      "Epoch 95/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3280 - accuracy: 0.8665 - val_loss: 0.3533 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3286 - accuracy: 0.8683 - val_loss: 0.3543 - val_accuracy: 0.8567\n",
      "Epoch 97/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8665 - val_loss: 0.3551 - val_accuracy: 0.8517\n",
      "Epoch 98/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 0.8656 - val_loss: 0.3532 - val_accuracy: 0.8580\n",
      "Epoch 99/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3277 - accuracy: 0.8661 - val_loss: 0.3589 - val_accuracy: 0.8458\n",
      "Epoch 100/100\n",
      "449/449 [==============================] - 1s 2ms/step - loss: 0.3283 - accuracy: 0.8659 - val_loss: 0.3537 - val_accuracy: 0.8576\n"
     ]
    }
   ],
   "source": [
    "classifier.compile(optimizer=\"RMSPROp\",loss='binary_crossentropy',metrics=['accuracy'])\n",
    "# fit ANN to the model\n",
    "model_history=classifier.fit(X_train,y_train,batch_size=10,epochs=100,validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c175e95-38fd-4373-a2bd-1ed5e9d6772e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdam is the best optimizers. If one wants to train the neural network in less time and more \\nefficiently than Adam is the optimizer.\\n\\nFor sparse data use the optimizers with dynamic learning rate.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q NO. 9 :\n",
    "\"\"\"\n",
    "Adam is the best optimizers. If one wants to train the neural network in less time and more \n",
    "efficiently than Adam is the optimizer.\n",
    "\n",
    "For sparse data use the optimizers with dynamic learning rate.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
